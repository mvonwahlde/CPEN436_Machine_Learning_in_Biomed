{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87454420",
   "metadata": {},
   "source": [
    "# CPEN 346 - Midterm Project\n",
    "\n",
    "Author: Matthew VonWahlde  \n",
    "Date: 10/12/2023  \n",
    "</nbsp>  \n",
    "In this project, I will implement logistic regression with regularization and apply it to the Wisconsin Diagnostic Breast Cancer (WDBC) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9af74ec",
   "metadata": {},
   "source": [
    "### Packages\n",
    "\n",
    "First, the necessary packages are imported into the project:  \n",
    "* Numpy - Implements parallel computing in arrays\n",
    "* Matplotlib - Convenient data plotting\n",
    "* Math - Useful mathematical functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bd32b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb792c4a",
   "metadata": {},
   "source": [
    "### Load Data from WDBC\n",
    "\n",
    "Firstly, the dataset from WDBC needs to be loaded into this project, so it can be used for training and testing. This dataset contains 569 examples of breast cancer tumors (357 benign, 212 malignant) with each having an ID, diagnosis, and 30 real-valued input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a6b44e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    \"\"\"\n",
    "    Loads and formats data from the WDBC dataset\n",
    "    \n",
    "    Args:\n",
    "      filename : relative path for the file that holds the data\n",
    "      \n",
    "    Returns:\n",
    "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "      y : (array_like Shape (m,)) outputs, 1 == malignant, 0 == benign\n",
    "    \"\"\"\n",
    "    # Load the data from the file\n",
    "    data = np.loadtxt(filename, dtype=str, delimiter=',')\n",
    "    \n",
    "    # Store the 30 features from each example into a 2D matrix and convert the type to float\n",
    "    X = data[:,2:32]\n",
    "    X = X.astype(float)\n",
    "    \n",
    "    # Store the outputs for each example and set each 'M' to a 1 and each 'B' to a 0\n",
    "    y_tmp = data[:,1]\n",
    "    numRows = y_tmp.shape[0]\n",
    "    y = np.zeros(numRows)\n",
    "    \n",
    "    for i in range(0, numRows):\n",
    "        # For each output, set to 1 if 'M' or 0 if 'B'\n",
    "        if y_tmp[i] == 'M':\n",
    "            y[i] = 1\n",
    "        else:\n",
    "            y[i] = 0\n",
    "    \n",
    "    # Return data and outputs\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a6740a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "X_train, y_train = load_data(\"./data/wdbc.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca226bd",
   "metadata": {},
   "source": [
    "### Z-Score Normalization\n",
    "\n",
    "To speed up the learning process, feature scaling will be implemented. In particular, Z-Score Normalization will be the method used to feature scale.  \n",
    "</nbsp>  \n",
    "Feature scaling is the act of altering the value of features, so they are all in a consistent range. If one particular feature is much larger than others, it will dominate when computing the output. Keeping a consistent scale among features will allow each feature to have significance when computing the prediction.  \n",
    "</nbsp>  \n",
    "Z-score normalization is a method of feature scaling that modifies each feature based on its mean and standard deviation.\n",
    "$$ x_{j,scaled} = \\frac{x_j - M_j}{\\sigma_j}$$\n",
    "This equation represents the equation to calculate each scaled feature. Note the M denotes the mean, $\\sigma$ denotes standard deviation, and j denotes the $j^{th}$ feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaaf7c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean_stddev(data):\n",
    "    \"\"\"\n",
    "    Calculates the mean and standard deviation for each feature in the data set\n",
    "    \n",
    "    Args:\n",
    "      data : (ndarray Shape (m,n)) data, m examples by n features\n",
    "      \n",
    "    Returns:\n",
    "      means     : (ndarray) mean of each feature\n",
    "      deviations: (ndarray) standard deviation of each feature\n",
    "    \"\"\"\n",
    "    # Rows and columns of the dataset\n",
    "    rows, cols = data.shape\n",
    "    \n",
    "    # Preallocate arrays for the mean and standard deviations\n",
    "    means = np.zeros(cols)\n",
    "    deviations = np.zeros(cols)\n",
    "    \n",
    "    # Calculate each mean and start deviation\n",
    "    for i in range(0, cols):\n",
    "        means[i] = np.mean(data[:,i])\n",
    "        deviations[i] = np.std(data[:,i])\n",
    "    \n",
    "    return means, deviations \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb9e28ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Z_score_norm(data):\n",
    "    \"\"\"\n",
    "    Feature scales the dataset using z-score normalization.\n",
    "    \n",
    "    Args:\n",
    "      data : (ndarray Shape (m,n)) data, m examples by n features\n",
    "      \n",
    "    Returns:\n",
    "      data : (ndarray Shape (m,n)) data, m examples by n features (z-score normalized)\n",
    "      dev\n",
    "    \"\"\"\n",
    "    # Calculate the mean and stardard deviation\n",
    "    mean, std_dev = calc_mean_stddev(X_train)\n",
    "    \n",
    "    rows, cols = data.shape\n",
    "\n",
    "    # For each feature in the dataset, perform z-score normalization\n",
    "    for i in range(0, cols):\n",
    "        data[:, i] = (data[:, i] - mean[i]) / std_dev[i]\n",
    "        \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7810b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use z-score normalization to feature scale\n",
    "X_train = Z_score_norm(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1401f32",
   "metadata": {},
   "source": [
    "### Sigmoid Function\n",
    "\n",
    "In logistic regression, the sigmoid function is used to \"squish\" the input data into a function that is between 0 and 1. When the input value heads toward negative infinity, the sigmoid function heads toward 0. When the input value heads toward positive infinity, the sigmoid function heads toward 1.  \n",
    "</nbsp>  \n",
    "$$ g(z) = \\frac{1}{1 + e^{-z}} \\quad 0 < g(z) < 1 $$  \n",
    "</nbsp>  \n",
    "The term z refers to the expression, $ \\vec{w} \\cdot \\vec{x} + b $, used in linear regression.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5a84445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Args:\n",
    "        z (ndarray): A scalar, numpy array of any size.\n",
    "\n",
    "    Returns:\n",
    "        g (ndarray): sigmoid(z), with the same shape as z\n",
    "         \n",
    "    \"\"\"\n",
    "    # Calculate the sigmoid output for each scalar input\n",
    "    g = 1 / (1 + np.exp(-1 * z))  \n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36270ab",
   "metadata": {},
   "source": [
    "### Cost Function for Logistic Regression with Regularization\n",
    "\n",
    "The cost function for logistic regression with regularization is similar to logistic expression, but with an extra term at the end.  \n",
    "</nbsp>\n",
    "$$ J(\\vec{w}, b) = \\frac{-1}{m} \\sum_{i=1}^{m} [ y^{(i)}log(f_{(\\vec{w}, b)}(\\vec{x}^{(i)})) + (1 - y^{(i)})log(f_{(\\vec{w}, b)}(\\vec{x}^{(i)})) ] + \\frac{\\lambda}{2m}\\sum_{j=1}^{n}w_j^2 $$\n",
    "$$ f_{\\vec{w},b}(\\vec{x}) = g(z) $$\n",
    "</nbsp>  \n",
    "This extra term is the regularization term, with the constant $\\lambda$ representing the regularization parameter. The addition of the regularization term helps balance two goals: fitting the data and keeping $w_j$ small. When $\\lambda$ is large, $w_j$ must be small to minimize cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f165844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b, lambda_):\n",
    "    \"\"\"\n",
    "    Computes the cost over all examples\n",
    "    \n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "      y : (array_like Shape (m,)) target value \n",
    "      w : (array_like Shape (n,)) Values of parameters of the model      \n",
    "      b : scalar Values of bias parameter of the model\n",
    "      lambda_: regularization constant\n",
    "      \n",
    "    Returns:\n",
    "      total_cost: (scalar)         cost \n",
    "    \"\"\"\n",
    "    # Get shape and set initial cost to 0\n",
    "    m, n = X.shape\n",
    "    total_cost = 0\n",
    "    \n",
    "    # For each example in the dataset\n",
    "    for i in range(0,m):\n",
    "        # Compute the output prediction\n",
    "        z = np.dot(w, X[i,:]) + b\n",
    "        g = sigmoid(z)\n",
    "        # Used to prevent floating point error from giving log function an input of 0\n",
    "        if (1-g) <= 0.00000000001:\n",
    "            g -= 0.00000000001\n",
    "        # Add cost for each example\n",
    "        total_cost += -1*y[i]*math.log(g) - (1-y[i])*math.log(1-g)\n",
    "    \n",
    "    total_cost /= m\n",
    "    \n",
    "    # Add regularization term\n",
    "    total_cost += ( lambda_ / (2*m) ) * np.dot(w, w)\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e7e522",
   "metadata": {},
   "source": [
    "### Computing the gradient\n",
    "\n",
    "Adding the regularization term adds one term to the end of the gradient function when calculating the derivative with respect to $w_j$, but the derivative with respect to b remains the same.  \n",
    "</nbsp>  \n",
    "$$ \\frac {\\partial J(\\vec{w},b)}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} [(f_{\\vec{w},b}(\\vec{x}^{(i)}) - y^{(i)})x_j^{(i)}] + \\frac{\\lambda}{m}w_j $$\n",
    "$$ \\frac {\\partial J(\\vec{w},b)}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} [(f_{\\vec{w},b}(\\vec{x}^{(i)}) - y^{(i)})] $$  \n",
    "$$ f_{\\vec{w},b}(\\vec{x}) = g(z) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba6fd5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b, lambda_): \n",
    "    \"\"\"\n",
    "    Computes the gradient for logistic regression \n",
    " \n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) variable such as house size \n",
    "      y : (array_like Shape (m,1)) actual value \n",
    "      w : (array_like Shape (n,1)) values of parameters of the model      \n",
    "      b : (scalar)                 value of parameter of the model \n",
    "      lambda_: regularization constant\n",
    "      \n",
    "    Returns\n",
    "      dj_dw: (array_like Shape (n,1)) The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db: (scalar)                The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    # Get shape and preallocate initial derivatives\n",
    "    m, n = X.shape\n",
    "    dj_dw = np.zeros(w.shape)\n",
    "    dj_db = 0.\n",
    "\n",
    "    # For each example, compute the change in the derivative for w and b\n",
    "    for i in range(0, m):\n",
    "        z = np.dot(w, X[i,:]) + b\n",
    "        g = sigmoid(z)\n",
    "\n",
    "        dj_dw += np.dot(g - y[i], X[i,:])\n",
    "        dj_db += g - y[i]\n",
    "       \n",
    "    dj_dw /= m\n",
    "    dj_db /= m\n",
    "    \n",
    "    # Add regularization term\n",
    "    dj_dw += (lambda_ / m) * w\n",
    "        \n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffe1124",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "The formula for gradient descent for logistic regression is as follows. Note that you should not update $w_j$ until b has been updated as to not use the new values of $w_j$ when computing b's gradient.  \n",
    "</nbsp>  \n",
    "repeat until convergence (or a set number of iterations) {  \n",
    "$ \\quad w_j = w_j - \\alpha \\frac{\\partial J(\\vec{w}, b)}{\\partial w_j} $  \n",
    "$ \\quad b = b - \\alpha \\frac{\\partial J(\\vec{w}, b)}{\\partial b} $  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00f121c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X :    (array_like Shape (m, n)\n",
    "      y :    (array_like Shape (m,))\n",
    "      w_in : (array_like Shape (n,))  Initial values of parameters of the model\n",
    "      b_in : (scalar)                 Initial value of parameter of the model\n",
    "      cost_function:                  function to compute cost\n",
    "      alpha : (float)                 Learning rate\n",
    "      num_iters : (int)               number of iterations to run gradient descent\n",
    "      lambda_ (scalar, float)         regularization constant\n",
    "      \n",
    "    Returns:\n",
    "      w : (array_like Shape (n,)) Updated values of parameters of the model after\n",
    "          running gradient descent\n",
    "      b : (scalar)                Updated value of parameter of the model after\n",
    "          running gradient descent\n",
    "    \"\"\"\n",
    "    \n",
    "    # number of training examples\n",
    "    m = len(X)\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w_history = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = compute_gradient(X, y, w_in, b_in, lambda_)   \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w_in = w_in - alpha * dj_dw               \n",
    "        b_in = b_in - alpha * dj_db              \n",
    "       \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            cost =  cost_function(X, y, w_in, b_in, lambda_)\n",
    "            J_history.append(cost)\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0 or i == (num_iters-1):\n",
    "            w_history.append(w_in)\n",
    "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")\n",
    "        \n",
    "    return w_in, b_in, J_history, w_history #return w and J,w history for graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6be01d",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "When testing out the logistic regression with regularization, you can alter the number of iterations, alpha, lambda, the random seed, and the initial b to see what provides the highest accuracy. In the future, it would be beneficial to use some data for training and some data for testing. This way, you can tell when the training data is overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09187f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost     2.66   \n",
      "Iteration 1000: Cost     0.05   \n",
      "Iteration 2000: Cost     0.05   \n",
      "Iteration 3000: Cost     0.05   \n",
      "Iteration 4000: Cost     0.05   \n",
      "Iteration 5000: Cost     0.05   \n",
      "Iteration 6000: Cost     0.05   \n",
      "Iteration 7000: Cost     0.05   \n",
      "Iteration 8000: Cost     0.05   \n",
      "Iteration 9000: Cost     0.05   \n",
      "Iteration 9999: Cost     0.05   \n"
     ]
    }
   ],
   "source": [
    "# Set random seed, inital w, and initial b\n",
    "np.random.seed(20)\n",
    "initial_w = 0.01 * (np.random.rand(30).reshape(-1) - 0.5)\n",
    "initial_b = 8\n",
    "\n",
    "# Some gradient descent settings\n",
    "iterations = 10000\n",
    "alpha = 1\n",
    "lambda_ = .1\n",
    "\n",
    "# Perform gradient descent\n",
    "w,b, J_history,_ = gradient_descent(X_train, y_train, initial_w, initial_b, \n",
    "                                   compute_cost, compute_gradient, alpha, iterations, lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ea6f931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cost_versus_iteration(J_hist):\n",
    "    \"\"\"\n",
    "    Creates a plot of the cost at each iteration of gradient descent\n",
    "    \n",
    "    Args:\n",
    "      J_hist : (ndarray) array of costs at each iteration\n",
    "      \n",
    "    Returns:\n",
    "      None\n",
    "    \"\"\"\n",
    "    # Plot the cost versus iteration\n",
    "    plt.plot(J_hist, c='b',label='Cost')\n",
    "    plt.title(\"Cost vs. iteration\"); \n",
    "    plt.ylabel('Cost')             ;  \n",
    "    plt.xlabel('iteration step')   ;  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7b32055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot cost versus iteration\n",
    "plot_cost_versus_iteration(J_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9778e535",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "In the following section, the accuracy of the model is tested, comparing the predicted output with the real diagnosis. The accuracy should be around 99.1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "682e4967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, b): \n",
    "    \"\"\"\n",
    "    Predict whether the label is 0 or 1 using learned logistic\n",
    "    regression parameters w\n",
    "    \n",
    "    Args:\n",
    "    X : (ndarray Shape (m, n))\n",
    "    w : (array_like Shape (n,))      Parameters of the model\n",
    "    b : (scalar, float)              Parameter of the model\n",
    "\n",
    "    Returns:\n",
    "    p: (ndarray (m,1))\n",
    "        The predictions for X using a threshold at 0.5\n",
    "    \"\"\"\n",
    "    # number of training examples\n",
    "    m, n = X.shape   \n",
    "    p = np.zeros(m)\n",
    "   \n",
    "    # For each example, give a prediction\n",
    "    for i in range(0, m):\n",
    "        z = np.dot(w, X[i,:]) + b\n",
    "        g = sigmoid(z)\n",
    "    \n",
    "        p[i] = 1 if g >= 0.5 else 0\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60fe907d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 99.121265\n"
     ]
    }
   ],
   "source": [
    "#Compute accuracy on our training set\n",
    "p = predict(X_train, w,b)\n",
    "print('Training Accuracy: %f'%(np.mean(p == y_train) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
